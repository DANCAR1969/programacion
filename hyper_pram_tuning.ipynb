{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DANCAR1969/programacion/blob/master/hyper_pram_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1JpsDEUOZSR",
        "outputId": "7401539f-7e5e-4c4e-a7cd-c04acd9ce219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
            "Mejores hiperparámetros:\n",
            "{'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Accuracy en el conjunto de prueba: 0.8100558659217877\n",
            "\n",
            "Reporte de Clasificación:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.90      0.85       105\n",
            "           1       0.83      0.68      0.75        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.82      0.79      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Default.DESKTOP-PEHND4S\\anaconda3\\envs\\manim\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
            "540 fits failed out of a total of 1620.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "154 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Default.DESKTOP-PEHND4S\\anaconda3\\envs\\manim\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\Default.DESKTOP-PEHND4S\\anaconda3\\envs\\manim\\Lib\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"c:\\Users\\Default.DESKTOP-PEHND4S\\anaconda3\\envs\\manim\\Lib\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"c:\\Users\\Default.DESKTOP-PEHND4S\\anaconda3\\envs\\manim\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "386 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Default.DESKTOP-PEHND4S\\anaconda3\\envs\\manim\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\Default.DESKTOP-PEHND4S\\anaconda3\\envs\\manim\\Lib\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"c:\\Users\\Default.DESKTOP-PEHND4S\\anaconda3\\envs\\manim\\Lib\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"c:\\Users\\Default.DESKTOP-PEHND4S\\anaconda3\\envs\\manim\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "c:\\Users\\Default.DESKTOP-PEHND4S\\anaconda3\\envs\\manim\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.79078105 0.80200926 0.81045011\n",
            " 0.81745297 0.82307692 0.82164877 0.82442628 0.82724318 0.83567418\n",
            " 0.81743327 0.82161923 0.82303753 0.81462622 0.81883187 0.82304738\n",
            " 0.83005023 0.83004038 0.83566434 0.82160938 0.82441643 0.82723333\n",
            " 0.82160938 0.82441643 0.82723333 0.82299813 0.83003053 0.83003053\n",
            " 0.79078105 0.80200926 0.81045011 0.81745297 0.82307692 0.82164877\n",
            " 0.82442628 0.82724318 0.83567418 0.81743327 0.82161923 0.82303753\n",
            " 0.81462622 0.81883187 0.82304738 0.83005023 0.83004038 0.83566434\n",
            " 0.82160938 0.82441643 0.82723333 0.82160938 0.82441643 0.82723333\n",
            " 0.82299813 0.83003053 0.83003053        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.83707279 0.83705309 0.83283758 0.83565449 0.83142913 0.83283758\n",
            " 0.82019108 0.82722348 0.82722348 0.83142913 0.82863193 0.83003053\n",
            " 0.83284743 0.82863193 0.83003053 0.82158968 0.82581503 0.82721363\n",
            " 0.82018123 0.82579533 0.82860238 0.82018123 0.82579533 0.82860238\n",
            " 0.82160938 0.82440658 0.82160938 0.83707279 0.83705309 0.83283758\n",
            " 0.83565449 0.83142913 0.83283758 0.82019108 0.82722348 0.82722348\n",
            " 0.83142913 0.82863193 0.83003053 0.83284743 0.82863193 0.83003053\n",
            " 0.82158968 0.82581503 0.82721363 0.82018123 0.82579533 0.82860238\n",
            " 0.82018123 0.82579533 0.82860238 0.82160938 0.82440658 0.82160938\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.81462622 0.82165862 0.82305722\n",
            " 0.82023047 0.82867133 0.83006008 0.82720378 0.83425588 0.83706294\n",
            " 0.81039102 0.81881217 0.82863193 0.81742342 0.82304738 0.83287698\n",
            " 0.82439673 0.83144883 0.83706294 0.82862208 0.83143898 0.83284743\n",
            " 0.82862208 0.83143898 0.83284743 0.83140944 0.82862208 0.82863193\n",
            " 0.81462622 0.82165862 0.82305722 0.82023047 0.82867133 0.83006008\n",
            " 0.82720378 0.83425588 0.83706294 0.81039102 0.81881217 0.82863193\n",
            " 0.81742342 0.82304738 0.83287698 0.82439673 0.83144883 0.83706294\n",
            " 0.82862208 0.83143898 0.83284743 0.82862208 0.83143898 0.83284743\n",
            " 0.83140944 0.82862208 0.82863193        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.79357825 0.80341771 0.80904166 0.81465577 0.82166847 0.82164877\n",
            " 0.82442628 0.82724318 0.83567418 0.81883187 0.82301783 0.82303753\n",
            " 0.81462622 0.81883187 0.82304738 0.83005023 0.83004038 0.83566434\n",
            " 0.82160938 0.82441643 0.82723333 0.82160938 0.82441643 0.82723333\n",
            " 0.82299813 0.83003053 0.83003053 0.79357825 0.80341771 0.80904166\n",
            " 0.81465577 0.82166847 0.82164877 0.82442628 0.82724318 0.83567418\n",
            " 0.81883187 0.82301783 0.82303753 0.81462622 0.81883187 0.82304738\n",
            " 0.83005023 0.83004038 0.83566434 0.82160938 0.82441643 0.82723333\n",
            " 0.82160938 0.82441643 0.82723333 0.82299813 0.83003053 0.83003053]\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Importar librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Cargar el dataset Titanic (disponible en seaborn)\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Seleccionar columnas de interés y limpiar datos:\n",
        "# Variables utilizadas: pclass, sex, age, fare; Objetivo: survived\n",
        "data = df[['survived', 'pclass', 'sex', 'age', 'fare']].copy()\n",
        "data = data.dropna(subset=['survived'])  # Asegurarse de tener la variable objetivo\n",
        "\n",
        "# Rellenar valores faltantes en 'age' y 'fare'\n",
        "data['age'] = data['age'].fillna(data['age'].median())\n",
        "data['fare'] = data['fare'].fillna(data['fare'].median())\n",
        "\n",
        "# Convertir la columna 'sex' a valores numéricos: male -> 0, female -> 1\n",
        "data['sex'] = data['sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Definir variables predictoras (X) y la variable objetivo (y)\n",
        "X = data[['pclass', 'sex', 'age', 'fare']]\n",
        "y = data['survived']\n",
        "\n",
        "# Dividir el dataset en conjunto de entrenamiento y prueba (80%-20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Nota: Para Random Forest no es estrictamente necesario escalar, pero si se desea:\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# Definir el grid de hiperparámetros a optimizar\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Inicializar el clasificador Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Configurar Grid Search con validación cruzada (cv=5)\n",
        "grid_search = GridSearchCV(estimator=rf,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy',\n",
        "                           n_jobs=-1,    # Usar todos los núcleos disponibles\n",
        "                           verbose=2)\n",
        "\n",
        "# Ejecutar Grid Search en el conjunto de entrenamiento\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Mostrar los mejores hiperparámetros encontrados\n",
        "print(\"Mejores hiperparámetros:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Evaluar el mejor modelo en el conjunto de prueba\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy en el conjunto de prueba:\", accuracy)\n",
        "print(\"\\nReporte de Clasificación:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiBCane2OZST",
        "outputId": "1bd2d933-8c08-4f9b-e4ed-f0d1491734b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\default.desktop-pehnd4s\\anaconda3\\envs\\manim\\lib\\site-packages (from scikit-learn) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\default.desktop-pehnd4s\\anaconda3\\envs\\manim\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
            "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
            "   ------------------ --------------------- 5.2/11.1 MB 31.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.1/11.1 MB 28.9 MB/s eta 0:00:00\n",
            "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
            "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.6.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12 (manim))",
      "language": "python",
      "name": "manim"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}